<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title> - Aphros的个人博客</title><meta name="Description" content="This is my cool site"><meta property="og:title" content="" />
<meta property="og:description" content="集成学习 概述 集成学习是一种机器学习方法，旨在通过结合多个基本模型（称为基学习器）的预测来提高整体性能。它通过将不同的学习算法组合在一起，以解" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/programming/06.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" /><meta property="article:section" content="posts" />

<meta property="og:site_name" content="Aphros的博客" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="集成学习 概述 集成学习是一种机器学习方法，旨在通过结合多个基本模型（称为基学习器）的预测来提高整体性能。它通过将不同的学习算法组合在一起，以解"/>
<meta name="application-name" content="Aphros的博客">
<meta name="apple-mobile-web-app-title" content="Aphros的博客"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="http://localhost:1313/posts/programming/06.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" /><link rel="prev" href="http://localhost:1313/posts/programming/07.%E8%81%9A%E7%B1%BB/" /><link rel="next" href="http://localhost:1313/posts/programming/05.%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/localhost:1313\/posts\/programming\/06.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0\/"
        },"genre": "posts","wordcount":  5396 ,
        "url": "http:\/\/localhost:1313\/posts\/programming\/06.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0\/","publisher": {
            "@type": "Organization",
            "name": ""},"author": {
                "@type": "Person",
                "name": "Aphros"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Aphros的个人博客">Aphros的博客</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Aphros的个人博客">Aphros的博客</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a></div>
    </div>
</header><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">Contents</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX"></h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="/" title="Author" rel="author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>Aphros</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="0001-01-01">0001-01-01</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;5396 words&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;11 minutes&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>Contents</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#概述">概述</a></li>
    <li><a href="#自助法-bagging">自助法 Bagging</a>
      <ul>
        <li><a href="#随机森林">随机森林</a></li>
      </ul>
    </li>
    <li><a href="#增强法-boosting">增强法 Boosting</a>
      <ul>
        <li><a href="#自适应增强-adaboost-算法">自适应增强 Adaboost 算法</a></li>
        <li><a href="#梯度提升树-gbdt">梯度提升树 GBDT</a></li>
        <li><a href="#xgboost">XGBoost</a></li>
        <li><a href="#lightgbm">LightGBM</a></li>
      </ul>
    </li>
    <li><a href="#堆叠法-stacking算法">堆叠法 Stacking算法</a></li>
    <li><a href="#组件学习器">组件学习器</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h1 id="集成学习">集成学习</h1>
<h2 id="概述">概述</h2>
<p><strong>集成学习</strong>是一种机器学习方法，旨在通过结合多个基本模型（称为基学习器）的预测来提高整体性能。它通过将不同的学习算法组合在一起，以解决单个模型可能无法很好解决的问题。集成学习的核心思想是多个弱模型的组合能够形成一个强大的集成模型，从而提高预测的准确性和鲁棒性。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="http://192.168.0.103:7788/images/2023/08/24/1460000022464559.jpg"
        data-srcset="http://192.168.0.103:7788/images/2023/08/24/1460000022464559.jpg, http://192.168.0.103:7788/images/2023/08/24/1460000022464559.jpg 1.5x, http://192.168.0.103:7788/images/2023/08/24/1460000022464559.jpg 2x"
        data-sizes="auto"
        alt="http://192.168.0.103:7788/images/2023/08/24/1460000022464559.jpg"
        title="img" /></p>
<p><strong>个体学习器</strong></p>
<ul>
<li><strong>基学习器（base learner）<strong>如果所有的个体学习器都是</strong>同质的</strong>，或者说都是<strong>同一个种类</strong>的，那么称这种个体学习器为基学习器</li>
<li><strong>组件学习器（component learner）<strong>如果所有的个体学习器都是</strong>异质的</strong>，或者说是<strong>不同种类</strong>的，那么称这种个体学习器为组件学习器</li>
<li><strong>弱学习器（weak learner）<strong>指泛化性能</strong>略优于</strong>随机猜测的学习器</li>
<li><strong>强学习器</strong>指泛化性能<strong>显著优于</strong>随机猜测的学习器</li>
</ul>
<p><strong>集成方法</strong></p>
<ul>
<li>自助法（Bagging）</li>
<li>增强法（Boosting）</li>
<li>堆叠法（Stacking）</li>
</ul>
<p><strong>组合方法</strong></p>
<ul>
<li>
<p><strong>平均法</strong>平均法主要用于<strong>数值型输出</strong>，分为简单平均法和加权平均法</p>
<ul>
<li>简单平均法：取基学习器输出的平均数作为最终的输出</li>
</ul>
<p>$$
H(x)=\frac{1}{M}\sum_i^Mh_i(x)
$$</p>
<ul>
<li>加权平均法：对每个基学习器的输出附以权重后相加
$$
H(X)=\frac{1}{M}\sum_i^Mw_ih_i(x)
$$
$w_i$为每一个基学习器的权重，$w_i&gt;0$且$\sum_i^Mw_i=1$</li>
</ul>
</li>
<li>
<p><strong>投票法</strong>投票法主要用于<strong>分类型输出</strong>，分为绝对多数投票法、相对多数投票法和加权投票法</p>
<ul>
<li>
<p>绝对多数投票法：要求一个类别标记只有得票超过半数才被判断为该类别，否则拒绝预测
$$
H(x)=\begin{cases}c_j\quad\quad\sum_{i=1}^Mh_i^j(x)&gt;0.5\sum_{k=1}^N\sum_{i=1}^Mh_i^k(x)\refuse\end{cases}
$$</p>
</li>
<li>
<p>相对多数投票法：只要一个类别的预测输出最多则预测为该类别，不会拒绝预测</p>
</li>
</ul>
<p>$$
H(x)=c_{argmax_j\sum_{i=1}^Mh_i^j(x)}
$$</p>
<ul>
<li>
<p>加权投票法：在相对多数投票法的基础上为基学习器对每个类别的预测输出添加了权重
$$
H(x)=c_{argmax_j\sum_{i=1}^Mw_ih_i^j(x)}
$$</p>
</li>
<li>
<p>硬投票（Hard Voting）：$h_i^j(x)$的取值只能为0和1</p>
</li>
<li>
<p>软投票（Soft Voting）：$h_i^j(x)$的取值为0到1的任意值</p>
</li>
</ul>
</li>
</ul>
<h2 id="自助法-bagging">自助法 Bagging</h2>
<p><strong>自助法(Bootstrap Aggregating)</strong>  的基本思想是通过从原始数据集中进行有放回抽样（bootstrap），创建多个子数据集，然后训练基学习器，最终进行预测。</p>
<ul>
<li>对于给定的包含 m 个样本的数据集 D ，有放回的随机采样 m 次，就得到一个同样包含 m 个样本的数据集$\hat{D}$</li>
<li>初始数据集 D 中约有 36.8% 的样本未出现在采样数据集$\hat{D}$中，可以被用来作为测试集，测试得到的结果被称为包外估计</li>
</ul>
<p>$$
\lim_{m\rightarrow\infty}(1-\frac{1}{m})^m=\frac{1}{e}\approx0.368
$$</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="http://192.168.0.103:7788/images/2023/08/24/1460000022464560.jpg"
        data-srcset="http://192.168.0.103:7788/images/2023/08/24/1460000022464560.jpg, http://192.168.0.103:7788/images/2023/08/24/1460000022464560.jpg 1.5x, http://192.168.0.103:7788/images/2023/08/24/1460000022464560.jpg 2x"
        data-sizes="auto"
        alt="http://192.168.0.103:7788/images/2023/08/24/1460000022464560.jpg"
        title="img" /></p>
<ul>
<li><strong>组合方法</strong>
<ul>
<li>分类任务则采用<strong>投票法</strong>，得票最多的分类类别为最终的类别</li>
<li>回归任务则采用<strong>平均法</strong>，对所有基学习器的预测结果取平均得到最终的预测值</li>
</ul>
</li>
</ul>
<h3 id="随机森林">随机森林</h3>
<p><strong>随机森林（Random Forest）<strong>是一种利用</strong>决策树</strong>作为<strong>基学习器</strong>的<strong>Bagging集成学习算法</strong>，除此之外，随机森林还在决策树的训练过程中引入了<strong>随机属性选择</strong>。</p>
<ul>
<li><strong>构建过程</strong>
<ul>
<li><strong>随机采样</strong>
<ul>
<li>采用自助采样法，对于给定的包含 m 个样本的数据集 D ，有放回的随机采样 m 次</li>
</ul>
</li>
<li><strong>随机属性</strong>
<ul>
<li>训练集的特征个数为 d ，每次仅选择 k(k&lt;d) 个属性构建决策树</li>
<li>一般推荐$k=\log_2d$</li>
</ul>
</li>
<li><strong>树的构建</strong>
<ul>
<li>根据随机采样以及随机属性构建决策树，不进行剪枝，构建的若干棵决策树组成随机森林</li>
</ul>
</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 加载数据集</span>
</span></span><span class="line"><span class="cl"><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 划分训练集和测试集</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建随机森林模型</span>
</span></span><span class="line"><span class="cl"><span class="n">random_forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在训练集上训练模型</span>
</span></span><span class="line"><span class="cl"><span class="n">random_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在测试集上进行预测</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 计算准确率</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 加载数据集</span>
</span></span><span class="line"><span class="cl"><span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 划分训练集和测试集</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建随机森林回归模型</span>
</span></span><span class="line"><span class="cl"><span class="n">random_forest</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在训练集上训练模型</span>
</span></span><span class="line"><span class="cl"><span class="n">random_forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在测试集上进行预测</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">random_forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 计算均方误差</span>
</span></span><span class="line"><span class="cl"><span class="n">mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Mean Squared Error: </span><span class="si">{</span><span class="n">mse</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="增强法-boosting">增强法 Boosting</h2>
<p><strong>增强法 Boosting</strong> 是一种集成学习方法，它通过迭代地训练一系列弱学习器，并将它们组合成一个强大的集成模型。Boosting 方法通过关注先前迭代中被错误分类的样本，逐步提高这些样本的权重，从而让后续的弱学习器更专注于这些难以分类的样本。这有助于最终提高整体模型的性能。</p>
<p><strong>构建过程</strong></p>
<ul>
<li>利用初始训练样本集训练得到一个基学习器</li>
<li>提高被基学习器误分的样本的权重，使得误分类样本在下一轮训练中得到更大的关注，利用调整后的样本训练得到下一个基学习器</li>
<li>重复上述步骤，直至得到M个学习器</li>
<li>将以上M个学习器加权结合
<ul>
<li>对于分类问题，采用有权重的投票方式</li>
<li>对于回归问题，采用加权平均得到预测值</li>
</ul>
</li>
</ul>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="http://192.168.0.103:7788/images/2023/08/24/1460000022464558.jpg"
        data-srcset="http://192.168.0.103:7788/images/2023/08/24/1460000022464558.jpg, http://192.168.0.103:7788/images/2023/08/24/1460000022464558.jpg 1.5x, http://192.168.0.103:7788/images/2023/08/24/1460000022464558.jpg 2x"
        data-sizes="auto"
        alt="http://192.168.0.103:7788/images/2023/08/24/1460000022464558.jpg"
        title="img" /></p>
<p><strong>常见方法</strong></p>
<ul>
<li><strong>AdaBoost (Adaptive Boosting)</strong>: AdaBoost 是最早引入的 Boosting 方法之一。它在每个迭代中调整样本的权重，使先前被错误分类的样本获得更高的权重，从而让后续的模型更专注于这些样本。AdaBoost 使用加权多数投票的方式整合各个弱学习器的预测结果。</li>
<li><strong>Gradient Boosting</strong>: Gradient Boosting 是另一种常见的 Boosting 方法，通过逐步拟合残差来构建模型。每个新的弱学习器试图纠正之前模型的错误，使得整体模型逐步改进。XGBoost 和 LightGBM 是基于 Gradient Boosting 的优化算法，它们在大规模数据集上表现出色。</li>
<li><strong>XGBoost (Extreme Gradient Boosting)</strong>: 一种高度优化的 Gradient Boosting 方法，通过改进了的正则化技术、并行处理和缺失值处理来提高性能。它在梯度提升的基础上引入了正则化项和一阶和二阶梯度信息，从而在训练过程中能够更好地控制模型的复杂度。XGBoost 在处理大规模数据集和高维数据时表现优异。</li>
<li><strong>LightGBM (Light Gradient Boosting Machine)</strong>: 另一种基于 Gradient Boosting 的优化算法，它在处理大规模数据集时具有很高的效率。LightGBM 通过基于直方图的技术来处理特征分箱（binning），并采用了 Leaf-wise 生长策略，这使得它能够更快地建立决策树。此外，LightGBM 还支持类别特征的自动编码，从而减少了特征工程的工作量。</li>
</ul>
<p><strong>Bagging、Boosting二者之间的区别</strong></p>
<ul>
<li>
<p><strong>抽样方法：</strong></p>
<ul>
<li>Bagging（Bootstrap Aggregating）：Bagging 使用有放回抽样（bootstrap）来创建多个子数据集，每个子数据集用于训练不同的基学习器。</li>
<li>Boosting：Boosting 也使用抽样，但样本的权重会根据先前迭代中的表现进行调整，使错误分类的样本获得更高的权重。</li>
</ul>
</li>
<li>
<p><strong>弱学习器的训练方式：</strong></p>
<ul>
<li>
<p>Bagging：Bagging 使用并行方式训练每个基学习器，各个基学习器之间是独立的。</p>
</li>
<li>
<p>Boosting：Boosting 是串行训练的，每个基学习器会根据先前学习器的表现进行调整，专注于难以分类的样本。</p>
</li>
</ul>
</li>
<li>
<p><strong>重点关注样本：</strong></p>
<ul>
<li>
<p>Bagging：Bagging 平等对待所有样本，不特别关注错误分类的样本。</p>
</li>
<li>
<p>Boosting：Boosting 重点关注先前迭代中错误分类的样本，通过逐步提高这些样本的权重来进行学习。</p>
</li>
</ul>
</li>
<li>
<p><strong>预测结果整合：</strong></p>
<ul>
<li>
<p>Bagging：Bagging 方法通常通过投票或平均的方式整合各个基学习器的预测结果。</p>
</li>
<li>
<p>Boosting：Boosting 方法通常通过加权和的方式整合各个基学习器的预测结果。</p>
</li>
</ul>
</li>
<li>
<p><strong>集成模型性能：</strong></p>
<ul>
<li>
<p>Bagging：Bagging 方法通常能够减少模型的方差，从而提高泛化性能。</p>
</li>
<li>
<p>Boosting：Boosting 方法倾向于减少模型的偏差，从而提高整体性能。</p>
</li>
</ul>
</li>
</ul>
<h3 id="自适应增强-adaboost-算法">自适应增强 Adaboost 算法</h3>
<p><strong>Adaboost (Adaptive Boosting)</strong> 是 Boosting 算法中最有代表性的一个。原始的 Adaboost 算法用于解决二分类问题。</p>
<p><strong>构建过程</strong></p>
<p>对于一个训练集
$$
T={(x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)}
$$
<strong>初始化样例权重</strong>为
$$
D_1=(w_{11},w_{12},\cdots,w_{1n})\
w_{1i}=\frac{1}{n}
$$
即初始化时每个样例的权重相等</p>
<p>对于每一轮训练，采用<strong>不放回抽样</strong>的方法，训练基学习器，基学习器的<strong>误差</strong>为$e_m$，根据误差确定该基学习器的<strong>权重系数</strong>$\alpha_m$
$$
\alpha_m=\frac{1}{2}\ln\frac{1-e_m}{e_m}
$$
随后<strong>更新样例权重</strong>
$$
D_{m+1}=(w_{m+1,1},w_{m+1,2},\cdots,w_{m+1,n})\
w_{m+1,i}=\frac{w_{m,i}}{Z_m}e^{-\alpha_my_ih_m(x_i)}
$$
其中$Z_m$为规范化因子
$$
Z_m=\sum_{i=1}^n w_{m,i}e^{-\alpha_my_ih_m(x_i)}
$$</p>
<p>根据构建的M个基学习器得到最终的学习器
$$
h_{f(x)}=sign(\sum_{m=1}^M\alpha_mh_m(x))
$$</p>
<blockquote>
<p><strong>如何理解Adaboost 算法</strong></p>
<ol>
<li>分类器越准确，权重系数越大</li>
</ol>
<p>$$
e_m=P(h_m(x_i)\not=y_i)\in(0,1)
$$</p>
<p>显然，$e_m$越接近0，$\alpha_m$越大；$e_m$越接近1，$\alpha_m$越小；$e_m&lt;0.5$时，表明基分类器不如随机猜测，此时$\alpha_m$为负。</p>
<ol start="2">
<li>增加分类错误样本的权重，减少分类正确样本的权重</li>
</ol>
<p>$h_m(x_i)$为样本的预测类别，$y_i$为样本的真实类别，取值范围均为-1和1</p>
<p>讨论$\alpha_m&gt;0$的情形</p>
<p>当$y_ih_m(x_i)$为$1$时，表示样本分类正确，系数$e^{-\alpha_my_ih_m(x_i)}&lt;1$，$w_{m+1,i}$增大</p>
<p>为$-1$时表示样本分类错误，系数$e^{-\alpha_my_ih_m(x_i)}&gt;1$，$w_{m+1,i}$减小</p>
<ol start="3">
<li>规范化因子$Z_m$保证$D_{m+1}$为一个概率分布</li>
</ol>
<p>显然有：
$$
\sum_{i=1}^Nw_{m+1,i}=1
$$</p>
<ol start="4">
<li>最终的学习器通过<strong>加权</strong>和<strong>符号函数</strong>生成了输出取值范围为-1和1的<strong>强分类器</strong></li>
</ol>
</blockquote>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 加载数据集</span>
</span></span><span class="line"><span class="cl"><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 划分训练集和测试集</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建 AdaBoost 分类器</span>
</span></span><span class="line"><span class="cl"><span class="n">adaboost</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在训练集上训练模型</span>
</span></span><span class="line"><span class="cl"><span class="n">adaboost</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在测试集上进行预测</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">adaboost</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 计算准确率</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="梯度提升树-gbdt">梯度提升树 GBDT</h3>
<p>**梯度提升树（Gradient Boosting Decision Tree，GBDT）**是一种基于决策树的集成学习方法，属于 Boosting 算法的一种。它通过迭代地构建多个决策树，并通过梯度下降来逐步减小残差（预测误差），从而提高整体模型的性能。</p>
<p><strong>构建过程</strong></p>
<ul>
<li>先建立一棵树，然后逐渐迭代，每次迭代过程中都增加一棵树，逐渐形成众多树模型集成的强评估器。</li>
<li>其核心在于每棵树学的是之前所有树结论和的<strong>残差</strong>，这里的残差是指真实值与已有预测值的差值。</li>
<li>对于 GBDT 而言，每个样本的预测结果都是上一个样本预测结果的加权求和。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 加载数据集</span>
</span></span><span class="line"><span class="cl"><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 划分训练集和测试集</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建 GBDT 分类器</span>
</span></span><span class="line"><span class="cl"><span class="n">gbdt</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在训练集上训练模型</span>
</span></span><span class="line"><span class="cl"><span class="n">gbdt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在测试集上进行预测</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">gbdt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 计算准确率</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="xgboost">XGBoost</h3>
<p>**XGBoost（eXtreme Gradient Boosting）**是一种基于梯度提升树的机器学习算法，专门设计用于处理结构化数据和回归/分类问题。XGBoost 在梯度提升树的基础上引入了许多创新，以提高模型性能和训练速度。</p>
<p><strong>关键特点</strong></p>
<ul>
<li><strong>正则化项</strong>添加了正则化项（L1 和 L2 正则化）来控制模型的复杂度，避免过拟合。</li>
<li><strong>二阶导数信息</strong>利用二阶导数信息来更精确地拟合残差，提高预测的准确性。</li>
<li><strong>直方图近似</strong>使用直方图方法对特征分箱（binning），从而减少了计算和内存开销。</li>
<li><strong>缺失值处理</strong>可以自动处理缺失值，不需要额外的预处理步骤。</li>
<li><strong>分布式计算</strong>支持分布式计算，可以在多个核心上并行训练模型，提高训练速度。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">xgboost</span> <span class="k">as</span> <span class="nn">xgb</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 加载数据集</span>
</span></span><span class="line"><span class="cl"><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 划分训练集和测试集</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 将数据转换为 XGBoost 的 DMatrix 格式</span>
</span></span><span class="line"><span class="cl"><span class="n">dtrain</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">dtest</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">DMatrix</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">y_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 定义参数</span>
</span></span><span class="line"><span class="cl"><span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;objective&#34;</span><span class="p">:</span> <span class="s2">&#34;multi:softmax&#34;</span><span class="p">,</span>   <span class="c1"># 多分类问题</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;num_class&#34;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>                 <span class="c1"># 类别数</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;max_depth&#34;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>                 <span class="c1"># 树的深度</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;eta&#34;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>                     <span class="c1"># 学习率</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;seed&#34;</span><span class="p">:</span> <span class="mi">42</span>                      <span class="c1"># 随机种子</span>
</span></span><span class="line"><span class="cl"><span class="p">}</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 训练模型</span>
</span></span><span class="line"><span class="cl"><span class="n">num_round</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># 迭代次数</span>
</span></span><span class="line"><span class="cl"><span class="n">xgb_model</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">dtrain</span><span class="p">,</span> <span class="n">num_round</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在测试集上进行预测</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">xgb_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">dtest</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 计算准确率</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 加载数据集</span>
</span></span><span class="line"><span class="cl"><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 划分训练集和测试集</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建 XGBClassifier 模型</span>
</span></span><span class="line"><span class="cl"><span class="n">xgb_classifier</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="s2">&#34;multi:softmax&#34;</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在训练集上训练模型</span>
</span></span><span class="line"><span class="cl"><span class="n">xgb_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在测试集上进行预测</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">xgb_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 计算准确率</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h3 id="lightgbm">LightGBM</h3>
<p>**LightGBM（Light Gradient Boosting Machine）**是一种高效的梯度提升树算法，专门设计用于处理大规模数据集和高维数据。它在性能和效率方面进行了优化，通过一些创新性的技术，能够在短时间内构建强大的预测模型。</p>
<p><strong>关键特点</strong></p>
<ul>
<li><strong>基于直方图的特征分箱</strong>：使用直方图的方法来对特征进行分箱，减少了特征工程的负担，同时提高了计算效率。</li>
<li><strong>Leaf-wise 生长策略</strong>：与传统的层次生长不同，LightGBM 采用了 Leaf-wise 策略，从树的叶节点开始进行生长，可以更快地构建深度树。</li>
<li><strong>GOSS（Gradient-based One-Side Sampling）</strong>：GOSS 是一种样本采样策略，它只选择梯度大的样本来进行训练，从而提高了训练速度。</li>
<li><strong>EFB（Exclusive Feature Bundling）</strong>：EFB 是一种特征选择技术，通过将相关性高的特征捆绑在一起，减少了冗余特征，提高了模型效率。</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">lightgbm</span> <span class="k">as</span> <span class="nn">lgb</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 加载数据集</span>
</span></span><span class="line"><span class="cl"><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 划分训练集和测试集</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建 LightGBM 分类器模型</span>
</span></span><span class="line"><span class="cl"><span class="n">lgb_classifier</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">LGBMClassifier</span><span class="p">(</span><span class="n">objective</span><span class="o">=</span><span class="s2">&#34;multiclass&#34;</span><span class="p">,</span> <span class="n">num_class</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在训练集上训练模型</span>
</span></span><span class="line"><span class="cl"><span class="n">lgb_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在测试集上进行预测</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">lgb_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 计算准确率</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="堆叠法-stacking算法">堆叠法 Stacking算法</h2>
<p><strong>Stacking</strong>方法又称为 Stacked Generalization，是一种基于分层模型组合的集成算法。</p>
<p><strong>基本思想</strong></p>
<ul>
<li>利用初级学习算法对原始数据集进行学习，同时生成一个新的数据集。</li>
<li>根据从初级学习算法生成的新数据集，利用次级学习算法学习并得到最终的输出。</li>
</ul>
<p>对于初级学习器，可以是相同类型也可以是不同类型的。在新的数据集中，初级学习器的输出被用作次级学习器的输入特征，初始样本的标记仍被用作次级学习器学习样本的标记。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="http://192.168.0.103:7788/images/2023/08/24/stacking.png"
        data-srcset="http://192.168.0.103:7788/images/2023/08/24/stacking.png, http://192.168.0.103:7788/images/2023/08/24/stacking.png 1.5x, http://192.168.0.103:7788/images/2023/08/24/stacking.png 2x"
        data-sizes="auto"
        alt="http://192.168.0.103:7788/images/2023/08/24/stacking.png"
        title="Stacking" /></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-PYTHON" data-lang="PYTHON"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">StackingClassifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 加载数据集</span>
</span></span><span class="line"><span class="cl"><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>
</span></span><span class="line"><span class="cl"><span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 划分训练集和测试集</span>
</span></span><span class="line"><span class="cl"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 定义基学习器</span>
</span></span><span class="line"><span class="cl"><span class="n">base_learners</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;rf&#34;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
</span></span><span class="line"><span class="cl">    <span class="p">(</span><span class="s2">&#34;svm&#34;</span><span class="p">,</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&#34;linear&#34;</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 定义元学习器</span>
</span></span><span class="line"><span class="cl"><span class="n">meta_learner</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 创建 Stacking 分类器模型</span>
</span></span><span class="line"><span class="cl"><span class="n">stacking_classifier</span> <span class="o">=</span> <span class="n">StackingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="n">base_learners</span><span class="p">,</span> <span class="n">final_estimator</span><span class="o">=</span><span class="n">meta_learner</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在训练集上训练模型</span>
</span></span><span class="line"><span class="cl"><span class="n">stacking_classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 在测试集上进行预测</span>
</span></span><span class="line"><span class="cl"><span class="n">y_pred</span> <span class="o">=</span> <span class="n">stacking_classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 计算准确率</span>
</span></span><span class="line"><span class="cl"><span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="组件学习器">组件学习器</h2>
<p>.如果所有的个体学习器都是<strong>异质的</strong>，或者说是<strong>不同种类</strong>的，那么称这种个体学习器为组件学习器</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.neural_network</span> <span class="kn">import</span> <span class="n">MLPClassifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 组件学习器、堆叠法分类器</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">mlxtend.classifier</span> <span class="kn">import</span> <span class="n">EnsembleVoteClassifier</span><span class="p">,</span> <span class="n">StackingClassifier</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.gridspec</span> <span class="k">as</span> <span class="nn">gridspec</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 绘制决策边界</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">mlxtend.plotting</span> <span class="kn">import</span> <span class="n">plot_decision_regions</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]],</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 逻辑回归</span>
</span></span><span class="line"><span class="cl"><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 堆叠法的初级评估器</span>
</span></span><span class="line"><span class="cl"><span class="n">meta</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 决策树</span>
</span></span><span class="line"><span class="cl"><span class="n">dtc</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># K近邻</span>
</span></span><span class="line"><span class="cl"><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 支持向量机</span>
</span></span><span class="line"><span class="cl"><span class="n">svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;scale&#39;</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 朴素贝叶斯</span>
</span></span><span class="line"><span class="cl"><span class="n">gnb</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 多层感知器</span>
</span></span><span class="line"><span class="cl"><span class="n">mlp</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;logistic&#39;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 随机森林</span>
</span></span><span class="line"><span class="cl"><span class="n">rfc</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 投票法</span>
</span></span><span class="line"><span class="cl"><span class="n">evc</span> <span class="o">=</span> <span class="n">EnsembleVoteClassifier</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">clfs</span><span class="o">=</span><span class="p">[</span><span class="n">lr</span><span class="p">,</span> <span class="n">dtc</span><span class="p">,</span> <span class="n">knn</span><span class="p">,</span> <span class="n">svc</span><span class="p">,</span> <span class="n">gnb</span><span class="p">,</span> <span class="n">mlp</span><span class="p">,</span> <span class="n">rfc</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 堆叠法</span>
</span></span><span class="line"><span class="cl"><span class="n">sc</span> <span class="o">=</span> <span class="n">StackingClassifier</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">classifiers</span><span class="o">=</span><span class="p">[</span><span class="n">lr</span> <span class="n">dtc</span><span class="p">,</span> <span class="n">knn</span><span class="p">,</span> <span class="n">svc</span><span class="p">,</span> <span class="n">gnb</span><span class="p">,</span> <span class="n">mlp</span><span class="p">,</span> <span class="n">rfc</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">    <span class="n">meta_classifier</span><span class="o">=</span><span class="n">meta</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">clf_names</span> <span class="o">=</span> <span class="p">[</span>
</span></span><span class="line"><span class="cl">    <span class="s1">&#39;Logistic Regression&#39;</span><span class="p">,</span> <span class="s1">&#39;Decision Tree&#39;</span><span class="p">,</span> <span class="s1">&#39;KNN&#39;</span><span class="p">,</span><span class="s1">&#39;SVC&#39;</span><span class="p">,</span> <span class="s1">&#39;Naive Bayes&#39;</span><span class="p">,</span> <span class="s1">&#39;MLP&#39;</span><span class="p">,</span><span class="s1">&#39;Random Forest&#39;</span><span class="p">,</span> <span class="s1">&#39;Voting&#39;</span><span class="p">,</span> <span class="s1">&#39;Stacking&#39;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">clfs</span> <span class="o">=</span> <span class="p">[</span><span class="n">lr</span><span class="p">,</span> <span class="n">dtc</span><span class="p">,</span> <span class="n">knn</span><span class="p">,</span> <span class="n">svc</span><span class="p">,</span> <span class="n">gnb</span><span class="p">,</span> <span class="n">mlp</span><span class="p">,</span> <span class="n">rfc</span><span class="p">,</span> <span class="n">evc</span><span class="p">,</span> <span class="n">sc</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
</span></span><span class="line"><span class="cl"><span class="n">gs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="o">.</span><span class="n">GridSpec</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">clf</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">grid</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">clfs</span><span class="p">,</span> <span class="n">clf_names</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">clfs</span><span class="p">))):</span>
</span></span><span class="line"><span class="cl">    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 定义绘制空间</span>
</span></span><span class="line"><span class="cl">    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">gs</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">grid</span> <span class="o">/</span> <span class="mi">3</span><span class="p">),</span> <span class="n">grid</span> <span class="o">%</span> <span class="mi">3</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 绘制决策边界</span>
</span></span><span class="line"><span class="cl">    <span class="n">plot_decision_regions</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">clf</span><span class="o">=</span><span class="n">clf</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 无坐标轴</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;off&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 紧密布局</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># 绘制标题</span>
</span></span><span class="line"><span class="cl">    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
</span></span></code></pre></div></div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>Updated on 0001-01-01</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="Share on Twitter" data-sharer="twitter" data-url="http://localhost:1313/posts/programming/06.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" data-title=""><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Facebook" data-sharer="facebook" data-url="http://localhost:1313/posts/programming/06.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Hacker News" data-sharer="hackernews" data-url="http://localhost:1313/posts/programming/06.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" data-title=""><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on Line" data-sharer="line" data-url="http://localhost:1313/posts/programming/06.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" data-title=""><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="Share on 微博" data-sharer="weibo" data-url="http://localhost:1313/posts/programming/06.%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" data-title=""><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">Back</a></span>&nbsp;|&nbsp;<span><a href="/">Home</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/posts/programming/07.%E8%81%9A%E7%B1%BB/" class="prev" rel="prev" title=""><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i></a>
            <a href="/posts/programming/05.%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" class="next" rel="next" title=""><i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
</article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.118.2">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2022 - 2023</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">Aphros</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css"><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"Copy to clipboard","maxShownLines":50},"comment":{},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":true,"left":"\\begin{equation}","right":"\\end{equation}"},{"display":true,"left":"\\begin{equation*}","right":"\\end{equation*}"},{"display":true,"left":"\\begin{align}","right":"\\end{align}"},{"display":true,"left":"\\begin{align*}","right":"\\end{align*}"},{"display":true,"left":"\\begin{alignat}","right":"\\end{alignat}"},{"display":true,"left":"\\begin{alignat*}","right":"\\end{alignat*}"},{"display":true,"left":"\\begin{gather}","right":"\\end{gather}"},{"display":true,"left":"\\begin{CD}","right":"\\end{CD}"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
